#!/bin/bash
set -e

echo "ğŸš€ Local Deployment - Coding Crew System"

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama not found. Please install Ollama first:"
    echo "   curl -fsSL https://ollama.ai/install.sh | sh"
    exit 1
fi

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "ğŸ”„ Starting Ollama service..."
    ollama serve &
    sleep 5
fi

# Pull required models if not present
echo "ğŸ“¥ Checking required models..."
if ! ollama list | grep -q "llama3.1:8b"; then
    echo "ğŸ“¥ Pulling llama3.1:8b..."
    ollama pull llama3.1:8b
fi

if ! ollama list | grep -q "qwen2.5-coder:1.5b-base"; then
    echo "ğŸ“¥ Pulling qwen2.5-coder:1.5b-base..."
    ollama pull qwen2.5-coder:1.5b-base
fi

# Create environment file if not exists
if [ ! -f .env ]; then
    echo "ğŸ“ Creating .env file..."
    cp .env.example .env
fi

# Create data directories
mkdir -p data logs

# Install dependencies
echo "ğŸ“¦ Installing dependencies..."
uv sync

# Start the application
echo "ğŸƒ Starting Coding Crew System..."
echo "ğŸŒ Access at: http://localhost:8000"
echo "ğŸ“Š Health check: http://localhost:8000/health"
echo "ğŸ“‹ API docs: http://localhost:8000/docs"
echo ""
echo "Press Ctrl+C to stop"

uv run uvicorn coding_crew.main:app --host 0.0.0.0 --port 8000 --reload